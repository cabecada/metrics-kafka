package ly.stealth.kafka.metrics;

import com.codahale.metrics.*;
import com.google.gson.Gson;
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Properties;
import java.util.SortedMap;
import java.util.UUID;
import java.util.concurrent.TimeUnit;

public class KafkaReporter extends ScheduledReporter {
    private static final Logger log = LoggerFactory.getLogger(KafkaReporter.class);

    private final Producer<String, String> kafkaProducer;
    private final String kafkaTopic;
    private final Gson mapper = new Gson();

    private KafkaReporter(MetricRegistry registry,
                          String name,
                          MetricFilter filter,
                          TimeUnit rateUnit,
                          TimeUnit durationUnit,
                          String kafkaTopic,
                          Properties kafkaProperties) {
        super(registry, name, filter, rateUnit, durationUnit);
        this.kafkaTopic = kafkaTopic;
        kafkaProducer = new Producer<String, String>(new ProducerConfig(kafkaProperties));
    }

    @Override
    public synchronized void report(SortedMap<String, Gauge> gauges,
                                    SortedMap<String, Counter> counters,
                                    SortedMap<String, Histogram> histograms,
                                    SortedMap<String, Meter> meters,
                                    SortedMap<String, Timer> timers) {
        log.info("Trying to report metrics to Kafka kafkaTopic {}", kafkaTopic);
        String report = mapper.toJson(new KafkaMetricsReport(gauges, counters, histograms, meters, timers));
        log.debug("Created metrics report: {}", report);
        kafkaProducer.send(new KeyedMessage<String, String>(kafkaTopic, report));
        log.info("Metrics were successfully reported to Kafka kafkaTopic {}", kafkaTopic);
    }
    
    public static class Builder {
        private String kafkaTopic;

        /** brokerList
         * This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas).
         * The socket connections for sending the actual data will be established based on the broker information returned in
         * the metadata. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a
         * subset of brokers.
         */
        private String brokerList;

        /** clientId
         * The client id is a user-specified string sent in each request to help trace calls. It should logically identify
         * the application making the request.
         */
        private String clientId = UUID.randomUUID().toString();

        /** synchronously
         * This parameter specifies whether the messages are sent asynchronously in a background thread.
         * Valid values are false for asynchronous send and true for synchronous send. By setting the producer
         * to async we allow batching together of requests (which is great for throughput) but open the possibility
         * of a failure of the client machine dropping unsent data.
         */
        private boolean synchronously = true;

        /** compressionCodec
         * This parameter allows you to specify the compression codec for all data generated by this producer.
         * When set to true gzip is used.  To override and use snappy you need to implement that as the default
         * codec for compression using SnappyCompressionCodec.codec instead of DefaultCompressionCodec.codec below.
         */
        private String compressionCodec = "gzip";

        /** batchSize
         * The number of messages to send in one batch when using async mode.
         * The producer will wait until either this number of messages are ready
         * to send or queue.buffer.max.ms is reached.
         */
        private int batchSize = 200;

        /** messageSendMaxRetries
         * This property will cause the producer to automatically retry a failed send request.
         * This property specifies the number of retries when such failures occur. Note that
         * setting a non-zero value here can lead to duplicates in the case of network errors
         * that cause a message to be sent but the acknowledgement to be lost.
         */
        private int messageSendMaxRetries = 3;

        /** requestRequiredAcks
         *  0) which means that the producer never waits for an acknowledgement from the broker (the same behavior as 0.7).
         *     This option provides the lowest latency but the weakest durability guarantees (some data will be lost when a server fails).
         *  1) which means that the producer gets an acknowledgement after the leader replica has received the data. This option provides
         *     better durability as the client waits until the server acknowledges the request as successful (only messages that were
         *     written to the now-dead leader but not yet replicated will be lost).
         * -1) which means that the producer gets an acknowledgement after all in-sync replicas have received the data. This option
         *     provides the best durability, we guarantee that no messages will be lost as long as at least one in sync replica remains.
         */
        private int requestRequiredAcks = -1;

        private MetricRegistry registry;
        private String name;
        private MetricFilter filter;
        private TimeUnit rateUnit;
        private TimeUnit durationUnit;

        public String getKafkaTopic() {
            return kafkaTopic;
        }

        public Builder setKafkaTopic(String kafkaTopic) {
            this.kafkaTopic = kafkaTopic;
            return this;
        }

        public String getBrokerList() {
            return brokerList;
        }

        public Builder setBrokerList(String brokerList) {
            this.brokerList = brokerList;
            return this;
        }

        public String getClientId() {
            return clientId;
        }

        public Builder setClientId(String clientId) {
            this.clientId = clientId;
            return this;
        }

        public boolean isSynchronously() {
            return synchronously;
        }

        public Builder setSynchronously(boolean synchronously) {
            this.synchronously = synchronously;
            return this;
        }

        public String getCompressionCodec() {
            return compressionCodec;
        }

        public Builder setCompressionCodec(String compressionCodec) {
            this.compressionCodec = compressionCodec;
            return this;
        }

        public int getBatchSize() {
            return batchSize;
        }

        public Builder setBatchSize(int batchSize) {
            this.batchSize = batchSize;
            return this;
        }

        public int getMessageSendMaxRetries() {
            return messageSendMaxRetries;
        }

        public Builder setMessageSendMaxRetries(int messageSendMaxRetries) {
            this.messageSendMaxRetries = messageSendMaxRetries;
            return this;
        }

        public int getRequestRequiredAcks() {
            return requestRequiredAcks;
        }

        public Builder setRequestRequiredAcks(int requestRequiredAcks) {
            this.requestRequiredAcks = requestRequiredAcks;
            return this;
        }

        public MetricRegistry getRegistry() {
            return registry;
        }

        public Builder setRegistry(MetricRegistry registry) {
            this.registry = registry;
            return this;
        }

        public String getName() {
            return name;
        }

        public Builder setName(String name) {
            this.name = name;
            return this;
        }

        public MetricFilter getFilter() {
            return filter;
        }

        public Builder setFilter(MetricFilter filter) {
            this.filter = filter;
            return this;
        }

        public TimeUnit getRateUnit() {
            return rateUnit;
        }

        public Builder setRateUnit(TimeUnit rateUnit) {
            this.rateUnit = rateUnit;
            return this;
        }

        public TimeUnit getDurationUnit() {
            return durationUnit;
        }

        public Builder setDurationUnit(TimeUnit durationUnit) {
            this.durationUnit = durationUnit;
            return this;
        }

        public KafkaReporter build() {
            Properties props = new Properties();
            props.put("compression.codec", compressionCodec);
            props.put("producer.type", synchronously ? "sync" : "async");
            props.put("metadata.broker.list", brokerList);
            props.put("batch.num.messages", batchSize);
            props.put("message.send.max.retries", messageSendMaxRetries);
            props.put("require.requred.acks",requestRequiredAcks);
            props.put("client.id", clientId);

            return new KafkaReporter(registry, name, filter, rateUnit, durationUnit, kafkaTopic, props);
        }
    }
}
